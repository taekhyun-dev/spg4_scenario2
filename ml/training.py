# ml/training.py
from torchmetrics import JaccardIndex
import torch
import torch.nn as nn
import torch.optim.lr_scheduler as lr_scheduler
from torch.amp import autocast, GradScaler
from .model import create_mobilenet, create_resnet9
from config import FEDPROX_MU

IMAGENET_CLASSES = 1000
CIFAR_CLASSES = 10

def train_model(model, global_state_dict, train_loader, epochs=1, lr=0.01, device='cuda', sim_logger=None):
        """
        ì‹¤ì œ PyTorch ëª¨ë¸ í•™ìŠµì„ ìˆ˜í–‰í•˜ëŠ” ë¸”ë¡œí‚¹(ë™ê¸°) í•¨ìˆ˜.
        asyncio ì´ë²¤íŠ¸ ë£¨í”„ë¥¼ ë§‰ì§€ ì•Šê¸° ìœ„í•´ ë³„ë„ì˜ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤.
        """
        try:
            loader_length = len(train_loader)
            sim_logger.info(f"âœ… [Train] ë°°ì¹˜ ê°œìˆ˜: {loader_length}")
            if loader_length == 0:
                sim_logger.error("âš ï¸ DataLoaderê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. Datasetì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
                return # ë˜ëŠ” ë‹¤ë¥¸ ì—ëŸ¬ ì²˜ë¦¬
        except Exception as e:
            sim_logger.error(f"âŒ DataLoaderì˜ ê¸¸ì´ë¥¼ í™•ì¸í•˜ëŠ” ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")

        # --- í•™ìŠµ íŒŒíŠ¸ ---
        model.to(device)
        model.train()

        # [FedProx] ë¹„êµìš© ê¸€ë¡œë²Œ ëª¨ë¸ (Gradient ë¶ˆí•„ìš”)
        # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ with torch.no_grad() ì•ˆì—ì„œ ìƒì„±í•˜ê±°ë‚˜ í•„ìš”í•  ë•Œë§Œ ë¡œë“œ
        global_model = create_resnet9(num_classes=CIFAR_CLASSES)
        global_model.load_state_dict(global_state_dict)
        global_model.to(device)
        global_model.eval() # ì¤‘ìš”: gradientê°€ íë¥´ì§€ ì•Šë„ë¡ ì„¤ì •

        for param in global_model.parameters():
            param.requires_grad = False

        # # ImageNet í•™ìŠµì€ ë³´í†µ Momentumê³¼ Weight Decayê°€ í•„ìˆ˜ì ì„
        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4)
        scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)
        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)

        samples_count = 0
        scaler = GradScaler()
    
        for epoch in range(epochs):
            sim_logger.info(f"              ì—í¬í¬ {epoch+1}/{epochs} ì§„í–‰ ì¤‘...")
            for images, labels in train_loader:
                images, labels = images.to(device), labels.to(device)
                optimizer.zero_grad()

                with torch.autocast(device_type="cuda", dtype=torch.float16):
                    outputs = model(images)
                    loss = criterion(outputs, labels)
                
                    # --- FedProx ì†ì‹¤ í•¨ìˆ˜ ìˆ˜ì • ë¶€ë¶„ ---
                    #     ê·¼ì ‘ í•­(Proximal Term) ê³„ì‚°: ||w - w^t||^2
                    prox_term = 0.0

                    # model.parameters() (w)ì™€ global_model.parameters() (w^t) ë¹„êµ
                    for local_param, global_param in zip(model.parameters(), global_model.parameters()):
                        # .detach()ë¥¼ ì‚¬ìš©í•˜ì—¬ w^tì˜ gradientê°€ ê³„ì‚°ë˜ì§€ ì•Šë„ë¡ í•¨
                        prox_term += torch.sum((local_param - global_param)**2)

                    # --- FedProx ì†ì‹¤ í•¨ìˆ˜ ìµœì¢… ê³„ì‚° ë¶€ë¶„ ---
                    #     ìµœì¢… ì†ì‹¤ ê³„ì‚°: Loss + (mu/2) * prox_term
                    total_loss = loss + (FEDPROX_MU / 2) * prox_term

                # loss.backward()
                scaler.scale(total_loss).backward()
                scaler.step(optimizer)
                scaler.update()

                samples_count += labels.size(0)
            
            scheduler.step()
            
        if sim_logger:
                sim_logger.info(f"             ğŸ§  í•™ìŠµ ì™„ë£Œ (Samples: {samples_count})")

        # ë©”ëª¨ë¦¬ ì •ë¦¬
        model.to('cpu')
        del global_model # ëª…ì‹œì  ì‚­ì œ
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        return samples_count

def evaluate_model(model_state_dict, data_loader, device):
    """ì£¼ì–´ì§„ ëª¨ë¸ ê°€ì¤‘ì¹˜ì™€ ë°ì´í„°ë¡œë”ë¡œ ì •í™•ë„ì™€ ì†ì‹¤ì„ í‰ê°€"""
    model = create_resnet9(num_classes=CIFAR_CLASSES)
    model.load_state_dict(model_state_dict)
    model.to(device)
    model.eval()
    
    criterion = nn.CrossEntropyLoss()

    jaccard = JaccardIndex(task="multiclass", num_classes=IMAGENET_CLASSES).to(device)

    total_loss = 0.0
    correct_1 = 0
    correct_5 = 0
    total = 0
    
    with torch.no_grad():
        for images, labels in data_loader:
            images, labels = images.to(device), labels.to(device)

            # [ìµœì í™”] ì¶”ë¡  ì‹œì—ë„ AMP ì‚¬ìš© ê°€ëŠ¥ (ì†ë„ í–¥ìƒ)
            with autocast():
                outputs = model(images)
                loss = criterion(outputs, labels)

            total_loss += loss.item()
            total += labels.size(0)

            # --- Top-k Accuracy ê³„ì‚° ---
            # ImageNetì€ Top-5ê°€ ì¤‘ìš”í•œ ì§€í‘œì„
            _, pred = outputs.topk(5, 1, True, True)
            pred = pred.t()
            correct = pred.eq(labels.view(1, -1).expand_as(pred))

            # Top-1
            correct_1 += correct[:1].reshape(-1).float().sum().item()
            # Top-5
            correct_5 += correct[:5].reshape(-1).float().sum().item()

            # mIoU (Top-1 ê¸°ì¤€)
            jaccard.update(pred[0], labels)
            
    acc1 = 100 * correct_1 / total
    acc5 = 100 * correct_5 / total
    avg_loss = total_loss / len(data_loader)
    miou = jaccard.compute().item() * 100

    model.to('cpu')
    
    return acc1, avg_loss, miou