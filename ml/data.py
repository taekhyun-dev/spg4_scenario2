import torch
import numpy as np
import torchvision.transforms as transforms
from torchvision import datasets
from torch.utils.data import DataLoader, Subset
from collections import Counter
import os

def get_cifar10_loaders(num_clients: int, dirichlet_alpha: float = 0.5, 
                        data_root: str = './data', batch_size_val: int = 256, num_workers: int = 8):
    """
    CIFAR-10 ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  Dirichlet ë¶„í¬(Non-IID)ì— ë”°ë¼ í´ë¼ì´ì–¸íŠ¸ë³„ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
    """
    
    # 1. CIFAR-10 ì „ìš© ì •ê·œí™” ê°’ (Mean, Std)
    CIFAR_MEAN = (0.4914, 0.4822, 0.4465)
    CIFAR_STD  = (0.2023, 0.1994, 0.2010)

    # 2. ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì •ì˜ (Resizing ì œê±° -> 32x32 ì›ë³¸ ì‚¬ìš©)
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4), # ë°ì´í„° ì¦ê°•
        transforms.RandomHorizontalFlip(),    # ë°ì´í„° ì¦ê°•
        transforms.ToTensor(),
        transforms.Normalize(CIFAR_MEAN, CIFAR_STD),
    ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(CIFAR_MEAN, CIFAR_STD),
    ])

    print(f"ğŸ“¥ [Data] CIFAR-10 ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘... (Root: {data_root})")
    
    # 3. ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ
    # (ìµœì´ˆ ì‹¤í–‰ ì‹œ ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ ë©ë‹ˆë‹¤)
    train_dataset = datasets.CIFAR10(root=data_root, train=True, download=True, transform=transform_train)
    test_dataset = datasets.CIFAR10(root=data_root, train=False, download=True, transform=transform_test)

    # 4. Dirichlet ë¶„í¬ë¥¼ ì´ìš©í•œ Non-IID ë°ì´í„° ë¶„í• 
    print(f"âš–ï¸ [Data] Dirichlet ë¶„í¬(alpha={dirichlet_alpha})ë¡œ ë°ì´í„° ë¶„í•  ì¤‘...")
    
    targets = np.array(train_dataset.targets) # ë ˆì´ë¸” ëª©ë¡
    num_classes = 10
    
    # ê° í´ë¼ì´ì–¸íŠ¸ê°€ ê°€ì§ˆ ë°ì´í„° ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸
    client_indices = [[] for _ in range(num_clients)]
    
    # í´ë˜ìŠ¤ë³„ë¡œ ìˆœíšŒí•˜ë©° ë¶„ë°°
    for k in range(num_classes):
        # í•´ë‹¹ í´ë˜ìŠ¤(k)ë¥¼ ê°€ì§„ ë°ì´í„°ì˜ ì¸ë±ìŠ¤ë“¤ë§Œ ì¶”ì¶œ
        idx_k = np.where(targets == k)[0]
        np.random.shuffle(idx_k)
        
        # Dirichlet ë¶„í¬ë¡œ ë¹„ìœ¨ ìƒì„±
        proportions = np.random.dirichlet(np.repeat(dirichlet_alpha, num_clients))
        
        # ë¹„ìœ¨ì„ ì •ê·œí™”í•˜ì—¬ ê°œìˆ˜ ë¶€ì¡± ë¬¸ì œ ë°©ì§€ (ì•„ì£¼ ì ì€ ê²½ìš° ë³´ì •)
        proportions = np.array([p * (len(idx_k) < num_clients / 10.0 and 1.0 / num_clients or 1) for p in proportions])
        proportions = proportions / proportions.sum()
        proportions = (np.cumsum(proportions) * len(idx_k)).astype(int)[:-1]
        
        # ë¶„í• ëœ ì¸ë±ìŠ¤ë¥¼ ê° í´ë¼ì´ì–¸íŠ¸ì—ê²Œ í• ë‹¹
        split_indices = np.split(idx_k, proportions)
        for i in range(num_clients):
            client_indices[i].extend(split_indices[i])

    # 5. Subset ìƒì„± ë° ë°ì´í„° í†µê³„ ê³„ì‚°
    client_subsets = []
    total_data_count = 0
    
    for i in range(num_clients):
        # ì¸ë±ìŠ¤ ì…”í”Œ (í´ë˜ìŠ¤ë³„ë¡œ ë­‰ì³ìˆì§€ ì•Šê²Œ)
        np.random.shuffle(client_indices[i])
        subset = Subset(train_dataset, client_indices[i])
        client_subsets.append(subset)
        total_data_count += len(client_indices[i])

    avg_data_count = total_data_count / num_clients

    # 6. Global Validation Loader ìƒì„±
    # ê²€ì¦ì€ ë°°ì¹˜ ì‚¬ì´ì¦ˆë¥¼ í¬ê²Œ(256), ì›Œì»¤ë„ ë„‰ë„‰í•˜ê²Œ(8) ì„¤ì •í•˜ì—¬ ì†ë„ ìµœì í™”
    val_loader = DataLoader(
        test_dataset, 
        batch_size=batch_size_val, 
        shuffle=False, 
        num_workers=num_workers, 
        pin_memory=True
    )

    # (ë””ë²„ê¹…) ë¶„í•  ê²°ê³¼ ìš”ì•½ ì¶œë ¥ (ì²« 5ê°œ ìœ„ì„±ë§Œ)
    print(f"ğŸ“Š ë¶„í•  ì™„ë£Œ: ì´ {total_data_count}ê°œ í•™ìŠµ ë°ì´í„° (ìœ„ì„±ë‹¹ í‰ê·  {avg_data_count:.1f}ê°œ)")
    for i in range(min(5, num_clients)):
        indices = client_indices[i]
        labels = [targets[idx] for idx in indices]
        counts = Counter(labels)
        print(f"  - SAT_{i}: {len(indices)} samples {dict(sorted(counts.items()))}")

    return avg_data_count, client_subsets, val_loader, train_dataset.classes